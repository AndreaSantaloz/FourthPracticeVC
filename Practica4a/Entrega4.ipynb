{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3ae83bd9",
   "metadata": {},
   "source": [
    "# Práctica 4.a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "390c725d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "import numpy as np\n",
    "import cv2  \n",
    "import math \n",
    "import os\n",
    "from collections import defaultdict\n",
    "from roboflow import Roboflow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72494c5a",
   "metadata": {},
   "source": [
    "## Deteción de personas y vehículos presentes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "f68b42e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 5 persons, 2 bicycles, 3 cars, 4 traffic lights, 36.7ms\n",
      "Clase --> car\n",
      "Confianza ---> 0.93\n",
      "Clase --> car\n",
      "Confianza ---> 0.81\n",
      "Clase --> person\n",
      "Confianza ---> 0.7\n",
      "Clase --> car\n",
      "Confianza ---> 0.58\n",
      "Clase --> person\n",
      "Confianza ---> 0.38\n",
      "Clase --> person\n",
      "Confianza ---> 0.36\n",
      "Clase --> person\n",
      "Confianza ---> 0.33\n",
      "Clase --> person\n",
      "Confianza ---> 0.26\n",
      "Speed: 56.1ms preprocess, 36.7ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 5 persons, 3 bicycles, 3 cars, 4 traffic lights, 43.5ms\n",
      "Clase --> car\n",
      "Confianza ---> 0.93\n",
      "Clase --> car\n",
      "Confianza ---> 0.81\n",
      "Clase --> person\n",
      "Confianza ---> 0.7\n",
      "Clase --> car\n",
      "Confianza ---> 0.59\n",
      "Clase --> person\n",
      "Confianza ---> 0.38\n",
      "Clase --> person\n",
      "Confianza ---> 0.37\n",
      "Clase --> person\n",
      "Confianza ---> 0.33\n",
      "Clase --> person\n",
      "Confianza ---> 0.27\n",
      "Speed: 6.4ms preprocess, 43.5ms inference, 5.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 4 persons, 3 bicycles, 3 cars, 4 traffic lights, 29.1ms\n",
      "Clase --> car\n",
      "Confianza ---> 0.93\n",
      "Clase --> car\n",
      "Confianza ---> 0.81\n",
      "Clase --> person\n",
      "Confianza ---> 0.7\n",
      "Clase --> car\n",
      "Confianza ---> 0.58\n",
      "Clase --> person\n",
      "Confianza ---> 0.38\n",
      "Clase --> person\n",
      "Confianza ---> 0.37\n",
      "Clase --> person\n",
      "Confianza ---> 0.33\n",
      "Speed: 3.8ms preprocess, 29.1ms inference, 2.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 4 persons, 3 bicycles, 3 cars, 4 traffic lights, 31.4ms\n",
      "Clase --> car\n",
      "Confianza ---> 0.89\n",
      "Clase --> car\n",
      "Confianza ---> 0.8\n",
      "Clase --> person\n",
      "Confianza ---> 0.69\n",
      "Clase --> car\n",
      "Confianza ---> 0.63\n",
      "Clase --> person\n",
      "Confianza ---> 0.35\n",
      "Clase --> person\n",
      "Confianza ---> 0.31\n",
      "Clase --> person\n",
      "Confianza ---> 0.3\n",
      "Speed: 4.3ms preprocess, 31.4ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 6 persons, 2 bicycles, 2 cars, 4 traffic lights, 25.8ms\n",
      "Clase --> car\n",
      "Confianza ---> 0.89\n",
      "Clase --> car\n",
      "Confianza ---> 0.78\n",
      "Clase --> person\n",
      "Confianza ---> 0.7\n",
      "Clase --> person\n",
      "Confianza ---> 0.36\n",
      "Clase --> person\n",
      "Confianza ---> 0.31\n",
      "Clase --> person\n",
      "Confianza ---> 0.3\n",
      "Clase --> person\n",
      "Confianza ---> 0.29\n",
      "Clase --> person\n",
      "Confianza ---> 0.27\n",
      "Speed: 3.9ms preprocess, 25.8ms inference, 6.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 persons, 1 bicycle, 2 cars, 4 traffic lights, 31.5ms\n",
      "Clase --> car\n",
      "Confianza ---> 0.89\n",
      "Clase --> person\n",
      "Confianza ---> 0.75\n",
      "Clase --> car\n",
      "Confianza ---> 0.75\n",
      "Clase --> person\n",
      "Confianza ---> 0.29\n",
      "Clase --> person\n",
      "Confianza ---> 0.27\n",
      "Speed: 3.1ms preprocess, 31.5ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 5 persons, 1 bicycle, 3 cars, 4 traffic lights, 15.3ms\n",
      "Clase --> car\n",
      "Confianza ---> 0.86\n",
      "Clase --> car\n",
      "Confianza ---> 0.81\n",
      "Clase --> person\n",
      "Confianza ---> 0.75\n",
      "Clase --> person\n",
      "Confianza ---> 0.63\n",
      "Clase --> car\n",
      "Confianza ---> 0.36\n",
      "Clase --> person\n",
      "Confianza ---> 0.34\n",
      "Clase --> person\n",
      "Confianza ---> 0.29\n",
      "Clase --> person\n",
      "Confianza ---> 0.29\n",
      "Speed: 2.5ms preprocess, 15.3ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 5 persons, 1 bicycle, 3 cars, 5 traffic lights, 14.8ms\n",
      "Clase --> car\n",
      "Confianza ---> 0.86\n",
      "Clase --> car\n",
      "Confianza ---> 0.79\n",
      "Clase --> person\n",
      "Confianza ---> 0.75\n",
      "Clase --> person\n",
      "Confianza ---> 0.75\n",
      "Clase --> car\n",
      "Confianza ---> 0.41\n",
      "Clase --> person\n",
      "Confianza ---> 0.35\n",
      "Clase --> person\n",
      "Confianza ---> 0.29\n",
      "Clase --> person\n",
      "Confianza ---> 0.29\n",
      "Speed: 2.4ms preprocess, 14.8ms inference, 2.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 5 persons, 1 bicycle, 3 cars, 4 traffic lights, 17.9ms\n",
      "Clase --> car\n",
      "Confianza ---> 0.89\n",
      "Clase --> car\n",
      "Confianza ---> 0.84\n",
      "Clase --> person\n",
      "Confianza ---> 0.72\n",
      "Clase --> person\n",
      "Confianza ---> 0.71\n",
      "Clase --> car\n",
      "Confianza ---> 0.49\n",
      "Clase --> person\n",
      "Confianza ---> 0.31\n",
      "Clase --> person\n",
      "Confianza ---> 0.27\n",
      "Clase --> person\n",
      "Confianza ---> 0.27\n",
      "Speed: 4.1ms preprocess, 17.9ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 6 persons, 1 bicycle, 3 cars, 4 traffic lights, 24.5ms\n",
      "Clase --> car\n",
      "Confianza ---> 0.92\n",
      "Clase --> car\n",
      "Confianza ---> 0.8\n",
      "Clase --> person\n",
      "Confianza ---> 0.69\n",
      "Clase --> person\n",
      "Confianza ---> 0.67\n",
      "Clase --> car\n",
      "Confianza ---> 0.54\n",
      "Clase --> person\n",
      "Confianza ---> 0.34\n",
      "Clase --> person\n",
      "Confianza ---> 0.32\n",
      "Clase --> person\n",
      "Confianza ---> 0.29\n",
      "Clase --> person\n",
      "Confianza ---> 0.28\n",
      "Speed: 2.9ms preprocess, 24.5ms inference, 2.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 5 persons, 1 bicycle, 3 cars, 4 traffic lights, 16.0ms\n",
      "Clase --> car\n",
      "Confianza ---> 0.86\n",
      "Clase --> car\n",
      "Confianza ---> 0.77\n",
      "Clase --> car\n",
      "Confianza ---> 0.75\n",
      "Clase --> person\n",
      "Confianza ---> 0.73\n",
      "Clase --> person\n",
      "Confianza ---> 0.68\n",
      "Clase --> person\n",
      "Confianza ---> 0.32\n",
      "Clase --> person\n",
      "Confianza ---> 0.28\n",
      "Clase --> person\n",
      "Confianza ---> 0.28\n",
      "Speed: 5.5ms preprocess, 16.0ms inference, 3.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 4 persons, 1 bicycle, 3 cars, 5 traffic lights, 16.9ms\n",
      "Clase --> car\n",
      "Confianza ---> 0.92\n",
      "Clase --> car\n",
      "Confianza ---> 0.75\n",
      "Clase --> person\n",
      "Confianza ---> 0.75\n",
      "Clase --> car\n",
      "Confianza ---> 0.74\n",
      "Clase --> person\n",
      "Confianza ---> 0.7\n",
      "Clase --> person\n",
      "Confianza ---> 0.33\n",
      "Clase --> person\n",
      "Confianza ---> 0.29\n",
      "Speed: 2.7ms preprocess, 16.9ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 7 persons, 1 bicycle, 3 cars, 4 traffic lights, 16.5ms\n",
      "Clase --> car\n",
      "Confianza ---> 0.9\n",
      "Clase --> car\n",
      "Confianza ---> 0.76\n",
      "Clase --> person\n",
      "Confianza ---> 0.74\n",
      "Clase --> car\n",
      "Confianza ---> 0.74\n",
      "Clase --> person\n",
      "Confianza ---> 0.68\n",
      "Clase --> person\n",
      "Confianza ---> 0.42\n",
      "Clase --> person\n",
      "Confianza ---> 0.42\n",
      "Clase --> person\n",
      "Confianza ---> 0.32\n",
      "Clase --> person\n",
      "Confianza ---> 0.29\n",
      "Clase --> person\n",
      "Confianza ---> 0.26\n",
      "Speed: 2.3ms preprocess, 16.5ms inference, 4.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 6 persons, 1 bicycle, 3 cars, 4 traffic lights, 16.6ms\n",
      "Clase --> person\n",
      "Confianza ---> 0.82\n",
      "Clase --> car\n",
      "Confianza ---> 0.8\n",
      "Clase --> person\n",
      "Confianza ---> 0.79\n",
      "Clase --> car\n",
      "Confianza ---> 0.77\n",
      "Clase --> car\n",
      "Confianza ---> 0.71\n",
      "Clase --> person\n",
      "Confianza ---> 0.56\n",
      "Clase --> person\n",
      "Confianza ---> 0.37\n",
      "Clase --> person\n",
      "Confianza ---> 0.31\n",
      "Clase --> person\n",
      "Confianza ---> 0.28\n",
      "Speed: 4.8ms preprocess, 16.6ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 7 persons, 1 bicycle, 3 cars, 4 traffic lights, 19.8ms\n",
      "Clase --> car\n",
      "Confianza ---> 0.9\n",
      "Clase --> person\n",
      "Confianza ---> 0.81\n",
      "Clase --> car\n",
      "Confianza ---> 0.78\n",
      "Clase --> person\n",
      "Confianza ---> 0.76\n",
      "Clase --> car\n",
      "Confianza ---> 0.6\n",
      "Clase --> person\n",
      "Confianza ---> 0.44\n",
      "Clase --> person\n",
      "Confianza ---> 0.39\n",
      "Clase --> person\n",
      "Confianza ---> 0.34\n",
      "Clase --> person\n",
      "Confianza ---> 0.29\n",
      "Clase --> person\n",
      "Confianza ---> 0.27\n",
      "Speed: 2.2ms preprocess, 19.8ms inference, 5.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 7 persons, 1 bicycle, 3 cars, 4 traffic lights, 14.5ms\n",
      "Clase --> car\n",
      "Confianza ---> 0.84\n",
      "Clase --> car\n",
      "Confianza ---> 0.84\n",
      "Clase --> person\n",
      "Confianza ---> 0.8\n",
      "Clase --> person\n",
      "Confianza ---> 0.74\n",
      "Clase --> person\n",
      "Confianza ---> 0.71\n",
      "Clase --> car\n",
      "Confianza ---> 0.64\n",
      "Clase --> person\n",
      "Confianza ---> 0.42\n",
      "Clase --> person\n",
      "Confianza ---> 0.35\n",
      "Clase --> person\n",
      "Confianza ---> 0.29\n",
      "Clase --> person\n",
      "Confianza ---> 0.28\n",
      "Speed: 3.4ms preprocess, 14.5ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 6 persons, 1 bicycle, 3 cars, 4 traffic lights, 16.6ms\n",
      "Clase --> person\n",
      "Confianza ---> 0.9\n",
      "Clase --> car\n",
      "Confianza ---> 0.86\n",
      "Clase --> car\n",
      "Confianza ---> 0.81\n",
      "Clase --> person\n",
      "Confianza ---> 0.76\n",
      "Clase --> person\n",
      "Confianza ---> 0.72\n",
      "Clase --> car\n",
      "Confianza ---> 0.64\n",
      "Clase --> person\n",
      "Confianza ---> 0.32\n",
      "Clase --> person\n",
      "Confianza ---> 0.31\n",
      "Clase --> person\n",
      "Confianza ---> 0.27\n",
      "Speed: 2.9ms preprocess, 16.6ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 6 persons, 1 bicycle, 3 cars, 4 traffic lights, 17.3ms\n",
      "Clase --> person\n",
      "Confianza ---> 0.89\n",
      "Clase --> car\n",
      "Confianza ---> 0.89\n",
      "Clase --> car\n",
      "Confianza ---> 0.86\n",
      "Clase --> person\n",
      "Confianza ---> 0.77\n",
      "Clase --> person\n",
      "Confianza ---> 0.72\n",
      "Clase --> car\n",
      "Confianza ---> 0.61\n",
      "Clase --> person\n",
      "Confianza ---> 0.32\n",
      "Clase --> person\n",
      "Confianza ---> 0.3\n",
      "Clase --> person\n",
      "Confianza ---> 0.28\n",
      "Speed: 4.7ms preprocess, 17.3ms inference, 2.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 7 persons, 1 bicycle, 3 cars, 4 traffic lights, 16.2ms\n",
      "Clase --> car\n",
      "Confianza ---> 0.89\n",
      "Clase --> person\n",
      "Confianza ---> 0.89\n",
      "Clase --> car\n",
      "Confianza ---> 0.83\n",
      "Clase --> person\n",
      "Confianza ---> 0.71\n",
      "Clase --> person\n",
      "Confianza ---> 0.69\n",
      "Clase --> car\n",
      "Confianza ---> 0.68\n",
      "Clase --> person\n",
      "Confianza ---> 0.55\n",
      "Clase --> person\n",
      "Confianza ---> 0.31\n",
      "Clase --> person\n",
      "Confianza ---> 0.26\n",
      "Clase --> person\n",
      "Confianza ---> 0.26\n",
      "Speed: 3.2ms preprocess, 16.2ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 6 persons, 1 bicycle, 4 cars, 4 traffic lights, 12.8ms\n",
      "Clase --> person\n",
      "Confianza ---> 0.88\n",
      "Clase --> car\n",
      "Confianza ---> 0.86\n",
      "Clase --> car\n",
      "Confianza ---> 0.74\n",
      "Clase --> car\n",
      "Confianza ---> 0.71\n",
      "Clase --> person\n",
      "Confianza ---> 0.69\n",
      "Clase --> person\n",
      "Confianza ---> 0.68\n",
      "Clase --> person\n",
      "Confianza ---> 0.54\n",
      "Clase --> person\n",
      "Confianza ---> 0.33\n",
      "Clase --> car\n",
      "Confianza ---> 0.28\n",
      "Clase --> person\n",
      "Confianza ---> 0.27\n",
      "Speed: 2.3ms preprocess, 12.8ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 9 persons, 1 bicycle, 3 cars, 1 truck, 4 traffic lights, 17.8ms\n",
      "Clase --> car\n",
      "Confianza ---> 0.89\n",
      "Clase --> person\n",
      "Confianza ---> 0.88\n",
      "Clase --> person\n",
      "Confianza ---> 0.72\n",
      "Clase --> person\n",
      "Confianza ---> 0.71\n",
      "Clase --> car\n",
      "Confianza ---> 0.7\n",
      "Clase --> person\n",
      "Confianza ---> 0.69\n",
      "Clase --> car\n",
      "Confianza ---> 0.51\n",
      "Clase --> person\n",
      "Confianza ---> 0.47\n",
      "Clase --> person\n",
      "Confianza ---> 0.36\n",
      "Clase --> person\n",
      "Confianza ---> 0.33\n",
      "Clase --> person\n",
      "Confianza ---> 0.28\n",
      "Clase --> person\n",
      "Confianza ---> 0.26\n",
      "Speed: 2.3ms preprocess, 17.8ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 8 persons, 1 bicycle, 2 cars, 1 truck, 4 traffic lights, 14.3ms\n",
      "Clase --> car\n",
      "Confianza ---> 0.92\n",
      "Clase --> person\n",
      "Confianza ---> 0.86\n",
      "Clase --> person\n",
      "Confianza ---> 0.8\n",
      "Clase --> car\n",
      "Confianza ---> 0.73\n",
      "Clase --> person\n",
      "Confianza ---> 0.72\n",
      "Clase --> person\n",
      "Confianza ---> 0.58\n",
      "Clase --> person\n",
      "Confianza ---> 0.58\n",
      "Clase --> person\n",
      "Confianza ---> 0.34\n",
      "Clase --> person\n",
      "Confianza ---> 0.32\n",
      "Clase --> person\n",
      "Confianza ---> 0.28\n",
      "Speed: 2.6ms preprocess, 14.3ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 6 persons, 1 bicycle, 2 cars, 1 truck, 4 traffic lights, 17.9ms\n",
      "Clase --> car\n",
      "Confianza ---> 0.92\n",
      "Clase --> person\n",
      "Confianza ---> 0.86\n",
      "Clase --> person\n",
      "Confianza ---> 0.77\n",
      "Clase --> car\n",
      "Confianza ---> 0.76\n",
      "Clase --> person\n",
      "Confianza ---> 0.7\n",
      "Clase --> person\n",
      "Confianza ---> 0.68\n",
      "Clase --> person\n",
      "Confianza ---> 0.58\n",
      "Clase --> person\n",
      "Confianza ---> 0.3\n",
      "Speed: 2.5ms preprocess, 17.9ms inference, 2.3ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    }
   ],
   "source": [
    "#Definimos el modelo para separar los objetos que son personas y coches\n",
    "model = YOLO(\"yolo11n.pt\")\n",
    "classes = {0:\"person\",2:\"car\"}\n",
    "\n",
    "capture_video = cv2.VideoCapture(\"video1.mp4\")\n",
    "while(True):\n",
    "    ret,frame_video = capture_video.read()\n",
    "    if  ret:\n",
    "        results = model(frame_video,stream=True)\n",
    "        #Detección empezamos\n",
    "        for frames in results:\n",
    "            boxes = frames.boxes\n",
    "            for box in boxes:\n",
    "                # Clase\n",
    "                cls = int(box.cls[0])\n",
    "                if cls not in classes.keys():\n",
    "                    continue\n",
    "                else:\n",
    "                    print(\"Clase -->\",classes[cls])\n",
    "                    x1, y1, x2, y2 = box.xyxy[0]\n",
    "                    x1, y1, x2, y2 = int(x1), int(y1), int(x2), int(y2) # convert to int values\n",
    "                        \n",
    "                    # Confianza\n",
    "                    confidence = math.ceil((box.conf[0]*100))/100\n",
    "                    print(\"Confianza --->\",confidence)\n",
    "                    escala = int((cls / len(classes)) * 255 * 3)\n",
    "                    if escala >= 255*2:\n",
    "                        R = 255\n",
    "                        G = 255\n",
    "                        B = escala - 255*2\n",
    "                    else:\n",
    "                        if escala >= 255:\n",
    "                            R = 255\n",
    "                            G = escala - 255\n",
    "                            B = 0\n",
    "                        else:\n",
    "                            R = escala\n",
    "                            G = 0\n",
    "                            B = 0\n",
    "\n",
    "                        # Dibuja el contenedor y clase\n",
    "                    cv2.rectangle(frame_video, (x1, y1), (x2, y2), (R, G, B), 3)\n",
    "                    cv2.putText(frame_video, classes[cls] , [x1, y1], cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 0, B), 2)\n",
    "            cv2.imshow('capture_video', frame_video)\n",
    "        if cv2.waitKey(20) == 27:\n",
    "            break\n",
    "# Libera el objeto de captura\n",
    "capture_video.release()\n",
    "# Destruye ventanas\n",
    "cv2.destroyAllWindows()\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f9c24ea",
   "metadata": {},
   "source": [
    "# Deteción de matrículas presentes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f4521b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading Roboflow workspace...\n",
      "loading Roboflow project...\n"
     ]
    }
   ],
   "source": [
    "\n",
    "rf = Roboflow(api_key=\"\")\n",
    "project = rf.workspace(\"licenseplate-s6fjf\").project(\"license-plate-xmnzu\")\n",
    "version = project.version(1)\n",
    "dataset = version.download(\"yolov11\")\n",
    "#Funciona\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1ff680e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = YOLO(\"yolo11n.pt\")\n",
    "data_path = \"./license-plate-1/data.yaml\"\n",
    "results = model.train(data=data_path,epochs=15,imgsz=640)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87515fce",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Definimos el modelo para separar los objetos que son personas y coches\n",
    "model = YOLO(\"./runs/detect/train/weights/best.pt\")\n",
    "classes = {0:\"licenseplate\",1:\"car\"}\n",
    "\n",
    "capture_video = cv2.VideoCapture(\"Video.mp4\")\n",
    "while(True):\n",
    "    ret,frame_video = capture_video.read()\n",
    "    if  ret:\n",
    "        results = model(frame_video,stream=True)\n",
    "        #Detección empezamos\n",
    "        for frames in results:\n",
    "            boxes = frames.boxes\n",
    "            for box in boxes:\n",
    "                # Clase\n",
    "                cls = int(box.cls[0])\n",
    "                if cls not in classes.keys():\n",
    "                    continue\n",
    "                else:\n",
    "                    print(\"Clase -->\",classes[cls])\n",
    "                    x1, y1, x2, y2 = box.xyxy[0]\n",
    "                    x1, y1, x2, y2 = int(x1), int(y1), int(x2), int(y2) # convert to int values\n",
    "                        \n",
    "                    # Confianza\n",
    "                    confidence = math.ceil((box.conf[0]*100))/100\n",
    "                    print(\"Confianza --->\",confidence)\n",
    "                    escala = int((cls / len(classes)) * 255 * 3)\n",
    "                    if escala >= 255*2:\n",
    "                        R = 255\n",
    "                        G = 255\n",
    "                        B = escala - 255*2\n",
    "                    else:\n",
    "                        if escala >= 255:\n",
    "                            R = 255\n",
    "                            G = escala - 255\n",
    "                            B = 0\n",
    "                        else:\n",
    "                            R = escala\n",
    "                            G = 0\n",
    "                            B = 0\n",
    "\n",
    "                        # Dibuja el contenedor y clase\n",
    "                    cv2.rectangle(frame_video, (x1, y1), (x2, y2), (R, G, B), 3)\n",
    "                    cv2.putText(frame_video, classes[cls] , [x1, y1], cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 0, B), 2)\n",
    "            cv2.imshow('capture_video', frame_video)\n",
    "        if cv2.waitKey(20) == 27:\n",
    "            break\n",
    "# Libera el objeto de captura\n",
    "capture_video.release()\n",
    "# Destruye ventanas\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c325142f",
   "metadata": {},
   "source": [
    "# Seguimiento de personas y coches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1ffcbc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "from ultralytics import YOLO\n",
    "import math\n",
    "from collections import defaultdict\n",
    "# Necesitas numpy para la parte de polylines, asegúrate de tenerlo instalado: pip install numpy\n",
    "import numpy as np \n",
    "\n",
    "# 1. Carga del modelo (yolo11n.pt, genérico para detectar personas y coches)\n",
    "model = YOLO('yolo11n.pt') \n",
    "\n",
    "# 2. Etiqueta de las distintas clases (quitamos 'bicycle')\n",
    "# COCO IDs: 0=\"person\", 2=\"car\"\n",
    "classNames = {0: \"person\", 2: \"car\"}\n",
    "\n",
    "# IDs de clases que queremos detectar\n",
    "CLASSES_TO_TRACK = [0, 2] \n",
    "\n",
    "# Captura desde el video\n",
    "vid = cv2.VideoCapture(\"video1.mp4\")\n",
    "\n",
    "# Estructuras para el Conteo Único\n",
    "contador_clases_unicas = {nombre: 0 for nombre in classNames.values()}\n",
    "tracker_ids_contados = set()\n",
    "COLOR_TEXTO_CONTEO = (0, 255, 255) # Amarillo brillante BGR\n",
    "\n",
    "# Para la trayectoria (manteniendo tu lógica)\n",
    "track_history = defaultdict(lambda: [])\n",
    "\n",
    "while(True): \n",
    "    ret, img = vid.read()\n",
    "    if ret: \n",
    "        results = model.track(img, persist=True, classes=CLASSES_TO_TRACK, tracker=\"bytetrack.yaml\", verbose=False)\n",
    "        if not results or results[0].boxes.id is None:\n",
    "            pass\n",
    "        else:\n",
    "           \n",
    "            boxes = results[0].boxes.xywh.cpu()\n",
    "            track_ids = results[0].boxes.id.int().cpu().tolist()\n",
    "            for box, track_id in zip(boxes, track_ids):\n",
    "                x1, y1, w, h = box.tolist() \n",
    "                x1, y1, x2, y2 = int(x1 - w/2), int(y1 - h/2), int(x1 + w/2), int(y1 + h/2)\n",
    "                cls_tensor = results[0].boxes.cls.cpu().numpy()\n",
    "                conf_tensor = results[0].boxes.conf.cpu().numpy()\n",
    "                try:\n",
    "                    box_index = results[0].boxes.id.int().cpu().tolist().index(track_id)\n",
    "                    cls = int(cls_tensor[box_index])\n",
    "                    confidence = math.ceil((conf_tensor[box_index]*100))/100\n",
    "                    clase_nombre = classNames[cls]\n",
    "                except ValueError:\n",
    "                    continue\n",
    "                if track_id not in tracker_ids_contados:\n",
    "                    contador_clases_unicas[clase_nombre] += 1\n",
    "                    tracker_ids_contados.add(track_id)\n",
    "                    \n",
    "                    \n",
    "                escala = int((cls / len(CLASSES_TO_TRACK)) * 255 * 3) \n",
    "                R, G, B = 0, 0, 0\n",
    "                \n",
    "                if escala >= 255*2:\n",
    "                    R, G, B = 255, 255, escala - 255*2\n",
    "                elif escala >= 255:\n",
    "                    R, G, B = 255, escala - 255, 0\n",
    "                else:\n",
    "                    R, G, B = escala, 0, 0\n",
    "                    \n",
    "                color = (R, G, B) # BGR\n",
    "                \n",
    "                cv2.rectangle(img, (x1, y1), (x2, y2), color, 3)\n",
    "                \n",
    "                etiqueta = f\"ID:{track_id} {clase_nombre} ({confidence:.2f})\"\n",
    "                cv2.putText(img, etiqueta, [x1, y1 - 10], cv2.FONT_HERSHEY_SIMPLEX, 0.7, color, 2)\n",
    "\n",
    "                track = track_history[track_id]\n",
    "                track.append((float(x1 + w/2), float(y1 + h/2))) \n",
    "                if len(track) > 30:\n",
    "                    track.pop(0)\n",
    "                points = np.hstack(track).astype(np.int32).reshape((-1, 1, 2))\n",
    "                cv2.polylines(img, [points], isClosed=False, color=(230, 230, 230), thickness=10)\n",
    "\n",
    "        y_offset = 30\n",
    "        cv2.putText(img, \"--- CONTEO UNICO ACUMULADO ---\", (10, y_offset), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "        y_offset += 30\n",
    "        \n",
    "        for nombre, cantidad in contador_clases_unicas.items():\n",
    "            texto_conteo = f\"Total Unicos {nombre}: {cantidad}\"\n",
    "            cv2.putText(img, texto_conteo, (10, y_offset), cv2.FONT_HERSHEY_SIMPLEX, 0.8, COLOR_TEXTO_CONTEO, 2, cv2.LINE_AA) \n",
    "            y_offset += 30\n",
    "\n",
    "\n",
    "        cv2.imshow('YOLO Tracking & Unique Count', img)\n",
    "    \n",
    "    if cv2.waitKey(20) == 27:\n",
    "        break\n",
    "    \n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "vid.release()\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "print(\"\\n--- RESUMEN FINAL DE CONTEO DE OBJETOS ÚNICOS ---\")\n",
    "for nombre, cantidad in contador_clases_unicas.items():\n",
    "    print(f\"Total acumulado de objetos únicos ({nombre}) detectados: {cantidad}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97a61543",
   "metadata": {},
   "source": [
    "# Seguimiento de matriculas y coches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cec5c825",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "from ultralytics import YOLO\n",
    "import math\n",
    "from collections import defaultdict\n",
    "import numpy as np \n",
    "import os \n",
    "\n",
    "MODELO_PATH = \"./runs/detect/train/weights/best.pt\"\n",
    "\n",
    "model = YOLO(MODELO_PATH) \n",
    "\n",
    "\n",
    "\n",
    "classes = {0: \"Matricula\", 1: \"Coche\"}\n",
    "CLASSES_TO_TRACK = list(classes.keys()) \n",
    "VID_PATH = \"Video.mp4\"\n",
    "capture_video = cv2.VideoCapture(VID_PATH)\n",
    "contador_clases_unicas = {nombre: 0 for nombre in classes.values()}\n",
    "tracker_ids_contados = set()\n",
    "\n",
    "\n",
    "COLOR_TEXTO_CONTEO = (0, 255, 255) \n",
    "track_history = defaultdict(lambda: []) \n",
    "\n",
    "\n",
    "while(True): \n",
    "    ret, img = capture_video.read()\n",
    "\n",
    "    if ret: \n",
    "        results = model.track(\n",
    "            img, \n",
    "            persist=True, \n",
    "            classes=CLASSES_TO_TRACK, \n",
    "            tracker=\"bytetrack.yaml\", \n",
    "            verbose=False\n",
    "        )\n",
    "\n",
    "    \n",
    "        if not results or results[0].boxes.id is None:\n",
    "            pass \n",
    "        else:\n",
    "            boxes_data = results[0].boxes \n",
    "            \n",
    "            for i, box in enumerate(boxes_data):\n",
    "                cls = int(box.cls[0])\n",
    "                conf = box.conf[0].item()\n",
    "                track_id = int(box.id[0].item()) if box.id is not None else None\n",
    "                \n",
    "                x1, y1, x2, y2 = [int(val) for val in box.xyxy[0].tolist()]\n",
    "                \n",
    "                if cls in classes.keys():\n",
    "                    clase_nombre = classes[cls]\n",
    "                    \n",
    "                \n",
    "                    if track_id is not None and track_id not in tracker_ids_contados:\n",
    "                        contador_clases_unicas[clase_nombre] += 1\n",
    "                        tracker_ids_contados.add(track_id)\n",
    "                      \n",
    "                    escala = int((cls / len(classes)) * 255 * 3)\n",
    "                    R, G, B = 0, 0, 0\n",
    "                    if escala >= 255*2:\n",
    "                        R, G, B = 255, 255, escala - 255*2\n",
    "                    elif escala >= 255:\n",
    "                        R, G, B = 255, escala - 255, 0\n",
    "                    else:\n",
    "                        R, G, B = escala, 0, 0\n",
    "                    color = (R, G, B) \n",
    "                    \n",
    "                    \n",
    "                    cv2.rectangle(img, (x1, y1), (x2, y2), color, 3)\n",
    "                    \n",
    "                    \n",
    "                    etiqueta = f\"ID:{track_id} {clase_nombre} ({conf*100:.0f}%)\"\n",
    "                    cv2.putText(img, etiqueta, [x1, y1 - 10], cv2.FONT_HERSHEY_SIMPLEX, 0.7, color, 2)\n",
    "                    \n",
    "                    \n",
    "                    center_x, center_y = (x1 + x2) // 2, (y1 + y2) // 2\n",
    "                    track = track_history[track_id]\n",
    "                    track.append((float(center_x), float(center_y)))\n",
    "                    if len(track) > 30: \n",
    "                        track.pop(0)\n",
    "                    points = np.hstack(track).astype(np.int32).reshape((-1, 1, 2))\n",
    "                    cv2.polylines(img, [points], isClosed=False, color=(230, 230, 230), thickness=5)\n",
    "\n",
    "\n",
    "    \n",
    "        y_offset = 30\n",
    "        cv2.putText(img, \"--- CONTEO UNICO ACUMULADO ---\", (10, y_offset), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "        y_offset += 30\n",
    "        \n",
    "        for nombre, cantidad in contador_clases_unicas.items():\n",
    "            texto_conteo = f\"Total Unicos {nombre}: {cantidad}\"\n",
    "            cv2.putText(img, texto_conteo, (10, y_offset), cv2.FONT_HERSHEY_SIMPLEX, 0.8, COLOR_TEXTO_CONTEO, 2, cv2.LINE_AA) \n",
    "            y_offset += 30\n",
    "\n",
    "        cv2.imshow('Deteccion y Conteo Unico de Trafico', img)\n",
    "    \n",
    "\n",
    "    if cv2.waitKey(20) == 27:\n",
    "        break   \n",
    "    \n",
    "    if not ret:\n",
    "        break\n",
    "capture_video.release()\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "print(\"\\n--- RESUMEN FINAL DE CONTEO DE OBJETOS ÚNICOS ---\")\n",
    "for nombre, cantidad in contador_clases_unicas.items():\n",
    "    print(f\"Total acumulado de objetos únicos ({nombre}) detectados: {cantidad}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f68d57d",
   "metadata": {},
   "source": [
    "## 4B "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "VC_P4",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
