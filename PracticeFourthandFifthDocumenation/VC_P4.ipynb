{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Paquetes necesarios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2  \n",
    "import math \n",
    "from ultralytics import YOLO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modelos preentrenados, visualizando con las utilidades de ultralytics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carga del modelo\n",
    "#model = YOLO('yolo11n.pt') #Contenedores\n",
    "#model = YOLO('yolo11n-seg.pt') #Máscaras\n",
    "model = YOLO('yolo11n-pose.pt')  #Pose\n",
    "\n",
    "#Para un vídeo \n",
    "filename = \"TGC23_PdH_C0056cut.mp4\"\n",
    "results = model(filename, show=True)\n",
    "\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Desde cámara, detección con yolo11, modelo nano. Visualización propia con OpenCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[KDownloading https://github.com/ultralytics/assets/releases/download/v8.3.0/yolo11n.pt to 'yolo11n.pt': 100% ━━━━━━━━━━━━ 5.4MB 16.4MB/s 0.3s.2s<0.3s4s\n",
      "\n",
      "0: 480x640 1 person, 105.7ms\n",
      "Confianza ---> 0.38\n",
      "Clase --> person\n",
      "Speed: 2.3ms preprocess, 105.7ms inference, 5.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 25.7ms\n",
      "Confianza ---> 0.82\n",
      "Clase --> person\n",
      "Speed: 3.0ms preprocess, 25.7ms inference, 3.3ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 24.2ms\n",
      "Confianza ---> 0.35\n",
      "Clase --> person\n",
      "Confianza ---> 0.29\n",
      "Clase --> person\n",
      "Speed: 3.4ms preprocess, 24.2ms inference, 2.8ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 23.4ms\n",
      "Confianza ---> 0.73\n",
      "Clase --> person\n",
      "Speed: 3.1ms preprocess, 23.4ms inference, 2.7ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 25.6ms\n",
      "Confianza ---> 0.42\n",
      "Clase --> person\n",
      "Confianza ---> 0.26\n",
      "Clase --> person\n",
      "Speed: 2.7ms preprocess, 25.6ms inference, 2.9ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 33.1ms\n",
      "Confianza ---> 0.81\n",
      "Clase --> person\n",
      "Speed: 2.6ms preprocess, 33.1ms inference, 2.7ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 22.7ms\n",
      "Confianza ---> 0.76\n",
      "Clase --> person\n",
      "Speed: 2.5ms preprocess, 22.7ms inference, 2.7ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 25.4ms\n",
      "Confianza ---> 0.7\n",
      "Clase --> person\n",
      "Speed: 3.7ms preprocess, 25.4ms inference, 3.3ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 24.5ms\n",
      "Confianza ---> 0.51\n",
      "Clase --> person\n",
      "Speed: 4.9ms preprocess, 24.5ms inference, 2.9ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 29.9ms\n",
      "Confianza ---> 0.52\n",
      "Clase --> person\n",
      "Speed: 3.2ms preprocess, 29.9ms inference, 2.7ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 24.7ms\n",
      "Confianza ---> 0.57\n",
      "Clase --> person\n",
      "Speed: 3.7ms preprocess, 24.7ms inference, 3.7ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 24.1ms\n",
      "Confianza ---> 0.61\n",
      "Clase --> person\n",
      "Speed: 2.3ms preprocess, 24.1ms inference, 2.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 24.0ms\n",
      "Confianza ---> 0.72\n",
      "Clase --> person\n",
      "Speed: 2.4ms preprocess, 24.0ms inference, 3.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 24.9ms\n",
      "Confianza ---> 0.34\n",
      "Clase --> person\n",
      "Confianza ---> 0.28\n",
      "Clase --> person\n",
      "Speed: 2.4ms preprocess, 24.9ms inference, 2.8ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 23.5ms\n",
      "Confianza ---> 0.49\n",
      "Clase --> person\n",
      "Confianza ---> 0.29\n",
      "Clase --> person\n",
      "Speed: 3.0ms preprocess, 23.5ms inference, 3.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 23.9ms\n",
      "Confianza ---> 0.51\n",
      "Clase --> person\n",
      "Confianza ---> 0.31\n",
      "Clase --> person\n",
      "Speed: 2.4ms preprocess, 23.9ms inference, 2.7ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 28.7ms\n",
      "Confianza ---> 0.37\n",
      "Clase --> person\n",
      "Confianza ---> 0.27\n",
      "Clase --> person\n",
      "Speed: 2.6ms preprocess, 28.7ms inference, 3.8ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 22.8ms\n",
      "Confianza ---> 0.47\n",
      "Clase --> person\n",
      "Speed: 2.5ms preprocess, 22.8ms inference, 2.9ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 23.5ms\n",
      "Confianza ---> 0.42\n",
      "Clase --> person\n",
      "Speed: 2.8ms preprocess, 23.5ms inference, 3.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 28.3ms\n",
      "Confianza ---> 0.45\n",
      "Clase --> person\n",
      "Speed: 3.1ms preprocess, 28.3ms inference, 2.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 persons, 23.8ms\n",
      "Confianza ---> 0.38\n",
      "Clase --> person\n",
      "Confianza ---> 0.28\n",
      "Clase --> person\n",
      "Speed: 4.8ms preprocess, 23.8ms inference, 2.9ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 26.0ms\n",
      "Confianza ---> 0.38\n",
      "Clase --> person\n",
      "Speed: 2.3ms preprocess, 26.0ms inference, 3.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 23.8ms\n",
      "Confianza ---> 0.76\n",
      "Clase --> person\n",
      "Speed: 2.4ms preprocess, 23.8ms inference, 3.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 24.1ms\n",
      "Confianza ---> 0.5\n",
      "Clase --> person\n",
      "Speed: 48.4ms preprocess, 24.1ms inference, 2.8ms postprocess per image at shape (1, 3, 480, 640)\n"
     ]
    }
   ],
   "source": [
    "# Carga del modelo, descarga en disco si no está presente en la carpeta\n",
    "model = YOLO('yolo11n.pt') #Contenedores\n",
    "\n",
    "# Etiqueta de las distintas clases\n",
    "classNames = [\"person\", \"bicycle\", \"car\", \"motorbike\", \"aeroplane\", \"bus\", \"train\", \"truck\", \"boat\",\n",
    "              \"traffic light\", \"fire hydrant\", \"stop sign\", \"parking meter\", \"bench\", \"bird\", \"cat\",\n",
    "              \"dog\", \"horse\", \"sheep\", \"cow\", \"elephant\", \"bear\", \"zebra\", \"giraffe\", \"backpack\", \"umbrella\",\n",
    "              \"handbag\", \"tie\", \"suitcase\", \"frisbee\", \"skis\", \"snowboard\", \"sports ball\", \"kite\", \"baseball bat\",\n",
    "              \"baseball glove\", \"skateboard\", \"surfboard\", \"tennis racket\", \"bottle\", \"wine glass\", \"cup\",\n",
    "              \"fork\", \"knife\", \"spoon\", \"bowl\", \"banana\", \"apple\", \"sandwich\", \"orange\", \"broccoli\",\n",
    "              \"carrot\", \"hot dog\", \"pizza\", \"donut\", \"cake\", \"chair\", \"sofa\", \"pottedplant\", \"bed\",\n",
    "              \"diningtable\", \"toilet\", \"tvmonitor\", \"laptop\", \"mouse\", \"remote\", \"keyboard\", \"cell phone\",\n",
    "              \"microwave\", \"oven\", \"toaster\", \"sink\", \"refrigerator\", \"book\", \"clock\", \"vase\", \"scissors\",\n",
    "              \"teddy bear\", \"hair drier\", \"toothbrush\"\n",
    "              ]\n",
    "\n",
    "\n",
    "# Captura desde la webcam\n",
    "vid = cv2.VideoCapture(0)\n",
    "  \n",
    "while(True):      \n",
    "    # fotograma a fotograma\n",
    "    ret, img = vid.read()\n",
    "  \n",
    "    # si hay imagen válida\n",
    "    if ret:  \n",
    "        # Detecta en la imagen\n",
    "        results = model(img, stream=True)\n",
    "        \n",
    "        # Para cada detección\n",
    "        for r in results:\n",
    "            boxes = r.boxes\n",
    "\n",
    "            for box in boxes:\n",
    "                # Contenedor\n",
    "                x1, y1, x2, y2 = box.xyxy[0]\n",
    "                x1, y1, x2, y2 = int(x1), int(y1), int(x2), int(y2) # convert to int values\n",
    "                \n",
    "                # Confianza\n",
    "                confidence = math.ceil((box.conf[0]*100))/100\n",
    "                print(\"Confianza --->\",confidence)\n",
    "\n",
    "                # Clase\n",
    "                cls = int(box.cls[0])\n",
    "                print(\"Clase -->\", classNames[cls])\n",
    "\n",
    "                # Convierte identificador numérico de clase a un color RGB\n",
    "                escala = int((cls / len(classNames)) * 255 * 3)\n",
    "                if escala >= 255*2:\n",
    "                    R = 255\n",
    "                    G = 255\n",
    "                    B = escala - 255*2\n",
    "                else:\n",
    "                    if escala >= 255:\n",
    "                        R = 255\n",
    "                        G = escala - 255\n",
    "                        B = 0\n",
    "                    else:\n",
    "                        R = escala\n",
    "                        G = 0\n",
    "                        B = 0\n",
    "\n",
    "                # Dibuja el contenedor y clase\n",
    "                cv2.rectangle(img, (x1, y1), (x2, y2), (R, G, B), 3)\n",
    "                cv2.putText(img, classNames[cls] , [x1, y1], cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 0, B), 2)\n",
    "\n",
    "        # Muestra fotograma\n",
    "        cv2.imshow('Vid', img)\n",
    "    \n",
    "    # Detenemos pulsado ESC\n",
    "    if cv2.waitKey(20) == 27:\n",
    "        break\n",
    "  \n",
    "# Libera el objeto de captura\n",
    "vid.release()\n",
    "# Destruye ventanas\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seguimiento. Requiere instalar lap con pip install lap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 480x640 1 person, 28.4ms\n",
      "Speed: 2.8ms preprocess, 28.4ms inference, 28.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Confianza ---> 0.59\n",
      "Clase --> person\n",
      "\n",
      "0: 480x640 1 person, 27.0ms\n",
      "Speed: 2.9ms preprocess, 27.0ms inference, 3.7ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Confianza ---> 0.44\n",
      "Clase --> person\n",
      "\n",
      "0: 480x640 1 person, 29.4ms\n",
      "Speed: 3.0ms preprocess, 29.4ms inference, 4.8ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Confianza ---> 0.42\n",
      "Clase --> person\n",
      "\n",
      "0: 480x640 1 person, 27.3ms\n",
      "Speed: 3.2ms preprocess, 27.3ms inference, 3.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Confianza ---> 0.58\n",
      "Clase --> person\n",
      "\n",
      "0: 480x640 2 persons, 27.0ms\n",
      "Speed: 3.4ms preprocess, 27.0ms inference, 3.8ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Confianza ---> 0.44\n",
      "Clase --> person\n",
      "Confianza ---> 0.4\n",
      "Clase --> person\n",
      "\n",
      "0: 480x640 2 persons, 27.1ms\n",
      "Speed: 2.9ms preprocess, 27.1ms inference, 3.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Confianza ---> 0.4\n",
      "Clase --> person\n",
      "Confianza ---> 0.37\n",
      "Clase --> person\n",
      "\n",
      "0: 480x640 2 persons, 27.1ms\n",
      "Speed: 3.1ms preprocess, 27.1ms inference, 3.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Confianza ---> 0.42\n",
      "Clase --> person\n",
      "Confianza ---> 0.47\n",
      "Clase --> person\n",
      "\n",
      "0: 480x640 2 persons, 27.1ms\n",
      "Speed: 2.9ms preprocess, 27.1ms inference, 3.3ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Confianza ---> 0.48\n",
      "Clase --> person\n",
      "Confianza ---> 0.4\n",
      "Clase --> person\n",
      "\n",
      "0: 480x640 2 persons, 34.1ms\n",
      "Speed: 3.2ms preprocess, 34.1ms inference, 5.3ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Confianza ---> 0.44\n",
      "Clase --> person\n",
      "Confianza ---> 0.35\n",
      "Clase --> person\n",
      "\n",
      "0: 480x640 2 persons, 28.1ms\n",
      "Speed: 3.7ms preprocess, 28.1ms inference, 3.8ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Confianza ---> 0.44\n",
      "Clase --> person\n",
      "Confianza ---> 0.32\n",
      "Clase --> person\n",
      "\n",
      "0: 480x640 2 persons, 31.6ms\n",
      "Speed: 5.2ms preprocess, 31.6ms inference, 4.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Confianza ---> 0.45\n",
      "Clase --> person\n",
      "Confianza ---> 0.37\n",
      "Clase --> person\n",
      "\n",
      "0: 480x640 2 persons, 27.0ms\n",
      "Speed: 3.4ms preprocess, 27.0ms inference, 3.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Confianza ---> 0.52\n",
      "Clase --> person\n",
      "Confianza ---> 0.44\n",
      "Clase --> person\n",
      "\n",
      "0: 480x640 2 persons, 27.1ms\n",
      "Speed: 3.4ms preprocess, 27.1ms inference, 3.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Confianza ---> 0.42\n",
      "Clase --> person\n",
      "Confianza ---> 0.45\n",
      "Clase --> person\n",
      "\n",
      "0: 480x640 2 persons, 31.2ms\n",
      "Speed: 3.0ms preprocess, 31.2ms inference, 5.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Confianza ---> 0.51\n",
      "Clase --> person\n",
      "Confianza ---> 0.42\n",
      "Clase --> person\n",
      "\n",
      "0: 480x640 2 persons, 53.3ms\n",
      "Speed: 4.7ms preprocess, 53.3ms inference, 8.9ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Confianza ---> 0.49\n",
      "Clase --> person\n",
      "Confianza ---> 0.36\n",
      "Clase --> person\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "\n",
    "# Carga del modelo, descarga en disco si no está presente en la carpeta\n",
    "model = YOLO('yolo11n.pt') #Contenedores\n",
    "\n",
    "# Etiqueta de las distintas clases\n",
    "classNames = [\"person\", \"bicycle\", \"car\"]\n",
    "\n",
    "\n",
    "# Captura desde la webcam\n",
    "vid = cv2.VideoCapture(0)\n",
    "track_history = defaultdict(lambda: [])\n",
    "  \n",
    "while(True):      \n",
    "    # fotograma a fotograma\n",
    "    ret, img = vid.read()\n",
    "  \n",
    "    # si hay imagen válida\n",
    "    if ret:  \n",
    "        # Seguimiento, con persistencia entre fotogramas\n",
    "        results = model.track(img, persist=True, classes = [0,1,2])\n",
    "\n",
    "        if 0:\n",
    "            if results is not None:\n",
    "                print(results[0])\n",
    "                boxes = results[0].boxes.xywh.cpu()\n",
    "                track_ids = results[0].boxes.id.int().cpu().tolist()\n",
    "                annotated_frame = results[0].plot()\n",
    "                for box, track_id in zip(boxes, track_ids):\n",
    "                    x, y, w, h = box\n",
    "                    track = track_history[track_id]\n",
    "                    track.append((float(x), float(y)))\n",
    "                    if len(track) > 30:\n",
    "                        track.pop(0)\n",
    "                    points = np.hstack(track).astype(np.int32).reshape((-1, 1, 2))\n",
    "                    cv2.polylines(annotated_frame, [points], isClosed=False, color=(230, 230, 230), thickness=10)\n",
    "                cv2.imshow(\"YOLO11 Tracking\", annotated_frame)\n",
    "                if cv2.waitKey(1) & 0xFF == ord(\"q\"):\n",
    "                    break\n",
    "        \n",
    "\n",
    "        \n",
    "        # Para cada detección\n",
    "        for r in results:\n",
    "            boxes = r.boxes\n",
    "\n",
    "            for box in boxes:\n",
    "                # Contenedor\n",
    "                x1, y1, x2, y2 = box.xyxy[0]\n",
    "                x1, y1, x2, y2 = int(x1), int(y1), int(x2), int(y2) # convert to int values\n",
    "\n",
    "                #Etiqueta de seguimiento\n",
    "                if box.id is not None:\n",
    "                    track_id = str(int(box.id[0].tolist()))\n",
    "                else:\n",
    "                    track_id = ''\n",
    "                \n",
    "                # Confianza\n",
    "                confidence = math.ceil((box.conf[0]*100))/100\n",
    "                print(\"Confianza --->\",confidence)\n",
    "\n",
    "                # Clase\n",
    "                cls = int(box.cls[0])\n",
    "                print(\"Clase -->\", classNames[cls])\n",
    "\n",
    "                # Convierte identificador numérico de clase a un color RGB\n",
    "                escala = int((cls / len(classNames)) * 255 * 3)\n",
    "                if escala >= 255*2:\n",
    "                    R = 255\n",
    "                    G = 255\n",
    "                    B = escala - 255*2\n",
    "                else:\n",
    "                    if escala >= 255:\n",
    "                        R = 255\n",
    "                        G = escala - 255\n",
    "                        B = 0\n",
    "                    else:\n",
    "                        R = escala\n",
    "                        G = 0\n",
    "                        B = 0\n",
    "\n",
    "                # Dibuja el contenedor y clase\n",
    "                cv2.rectangle(img, (x1, y1), (x2, y2), (R, G, B), 3)\n",
    "                cv2.putText(img, track_id + ' ' + classNames[cls] , [x1, y1], cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 0, B), 2)\n",
    "\n",
    "        # Muestra fotograma\n",
    "        cv2.imshow('Vid', img)\n",
    "    \n",
    "    # Detenemos pulsado ESC\n",
    "    if cv2.waitKey(20) == 27:\n",
    "        break\n",
    "  \n",
    "# Libera el objeto de captura\n",
    "vid.release()\n",
    "# Destruye ventanas\n",
    "cv2.destroyAllWindows()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Intregración con seguimiento (tracking)\n",
    "!!!!!!!!!Nota: he tenido que bajar a la versión de python 3.9.5 e instalar lap con pip install lap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carga del modelo\n",
    "model = YOLO('yolo11n.pt') #Contenedores\n",
    "#model = YOLO('yolov11n-seg.pt') #Máscaras\n",
    "#model = YOLO('yolo11n-pose.pt')  #Pose\n",
    "\n",
    "#Para un vídeo \n",
    "filename = \"TGC23_PdH_C0056cut.mp4\"\n",
    "results = model.track(source=filename, show=True)  # BoT-SORT tracker (por defecto)\n",
    "#results = model.track(source=filename, show=True, tracker=\"bytetrack.yaml\")  # ByteTrack tracker\n",
    "\n",
    "cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "yolo_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
